1. How long did your reddit_averages.py take with 
(1) the reddit-0 data set and effectively no work, (2) no schema specified and not caching (on reddit-2 for this and the rest), 
(3) with a schema but not caching, (4) with both a schema and caching the twice-used DataFrame? [The reddit-0 test is effectively measuring 
the Spark startup time, so we can see how long it takes to do the actual work on reddit-2 in the best/worst cases.]

reddit-0(for test, not really doing any work): 
real    0m0.087s
user    0m0.061s
sys     0m0.023s

reddit-2(no schema, no cache):
real    0m0.089s
user    0m0.049s
sys     0m0.022s

reddit-2(with schema, no cache):
real    0m0.082s
user    0m0.065s
sys     0m0.014s

reddit-2(with schema, with cache):
real    0m0.075s
user    0m0.056s
sys     0m0.012s


2. Based on the above, does it look like most of the time taken to process the reddit-2 data set is in reading the files, 
or calculating the averages?

Most of the time looks like it was taken in reading the files.


3. Where did you use .cache() in your wikipedia_popular.py? [Hint: the answer had better be “once”… but where?]

I used .cache() after completing my filtering steps, before the groupby() function. The filtering is necessary for many of the 
subsequent steps, so using the cache() before the groupby() and join() will improve the running time by reducing unecessary operations.






