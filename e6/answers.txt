In the A/B test analysis, do you feel like we're p-hacking? How comfortable are you coming to a conclusion at p < 0.05?

The testing practices we used seem robust. The analyses we did were appropriate for the data and data type. 
I would feel fairly comfortable coming to a conclusion at p < 0.05, as is the convention. But we must also consider
and be aware of the size and context of the study.



If we had done T-tests between each pair of sorting implementation results, how many tests would we run? If we looked for p < 0.05
in them, what would the probability be of having any false conclusions, just by chance? That's the effective p-value of the 
many-T-tests analysis. [We could have done a Bonferroni correction when doing multiple T-tests, which is a fancy way of saying “for m
tests, look for significance at a/m”.]

If we would have done t-tests for each pair of sorting algorithm, we would have (7*6)/2 = 21 tests. Hence, m = 21 and for 21 tests, 
the p value will be < 0.05/21 = 0.0023.



Give a ranking of the sorting implementations by speed, including which ones could not be distinguished. 
(i.e. which pairs could our experiment not conclude had different running times?)

We can rank from fastest to slowest: partition_sort, qs1, qs2/qs3, qs4/qs5, merge1. 
qs2 and qs3 had the identical average speed of 0.061. qs4 and qs4 had the identical average speed of 0.064